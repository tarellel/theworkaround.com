<!doctype html> <html lang="en"> <head> <meta charset="utf-8"/> <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover"/> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta http-equiv="Content-Language" content="en"> <title>Docker Swarm Persistent Storage | TheWorkAround</title> <meta property="og:title" content="Docker Swarm Persistent Storage"/> <meta name="author" content="Brandon"/> <meta property="og:locale" content="en_US"/> <meta name="description" content="Docker-swarm persistent storage using glusterFS (a network filesystem)"/> <meta property="og:description" content="Docker-swarm persistent storage using glusterFS (a network filesystem)"/> <link rel="canonical" href="https://theworkaround.com/2019/05/15/docker-swarm-persistent-storage/"/> <meta property="og:url" content="https://theworkaround.com/2019/05/15/docker-swarm-persistent-storage/"/> <meta property="og:site_name" content="TheWorkAround"/> <meta property="og:type" content="article"/> <meta property="article:published_time" content="2019-05-15T00:00:00+00:00"/> <meta name="twitter:card" content="summary"/> <meta property="twitter:title" content="Docker Swarm Persistent Storage"/> <link type="application/atom+xml" rel="alternate" href="https://theworkaround.com/feed.xml" title="TheWorkAround"/> <meta name="author" content="Brandon Hicks"> <meta name="description" content="The infinite ramblings and ideas of Brandon Hicks"/> <meta name="generator" content="Bridgetown"> <meta name="referrer" content="no-referrer-when-downgrade"> <meta name="robots" content="index, follow"> <link rel="preload" href="/_bridgetown/static/index.AIKPFYK3.css" as="style"> <link rel="preload" href="/_bridgetown/static/index.IGRG3H33.js" as="script"> <link rel="preconnect" href="/" crossorigin> <link rel="preconnect" href="https://rsms.me/inter/inter.css" crossorigin> <link rel="dns-prefetch" href="https://www.googletagmanager.com/"> <link rel="dns-prefetch" href="https://www.google-analytics.com/"> <link rel="dns-prefetch" href="https://ajax.googleapis.com/"> <link rel="stylesheet" href="https://rsms.me/inter/inter.css"> <link rel="stylesheet" href="/_bridgetown/static/index.AIKPFYK3.css" data-turbo-track="reload"/> <script src="/_bridgetown/static/index.IGRG3H33.js" data-turbo-track="reload" defer></script> <link href="https://theworkaround.com/sitemap.xml" rel="sitemap" title="Sitemap" type="application/xml"> <link href="https://theworkaround.com/humans.txt" rel="author" type="text/plain"> <link rel="manifest" href="https://theworkaround.com/site.webmanifest"> <link rel="me" href="https://theworkaround.com/" type="text/html"> <link rel="me" href="https://github.com/tarellel"> <link rel="me" href="https://twitter.com/tarellel"> <meta name="theme-color" content="#4cb8c4"> </head> <body class="post antialiased min-h-full font-inter font-normal leading-normal bg-white text-gray-400"> <nav class="border-t-12 border-cyan-600 py-3 flex flex-col sm:flex-row sm:justify-between sm:items-center p-4" role="navigation" aria-label="Main"> <div class="text-left"> <h3 class="text-3xl font-extrabold text-gray-600"> <a href="/" class="text-gray-600" title="Goto /" key="home">TheWorkAround</a> </h3> <p class="text-md font-semibold leading-5 text-gray-400">Brandon Hicks</p> </div> <div class="sm:text-right text-lg lg:text-xl flex space-y-4 sm:space-y-0 sm:space-x-9 content-center mt-3 sm:mt-0 flex-col sm:flex-row"> <a href="https://theworkaround.com/about" class="font-semibold text-gray-700 tracking-tight" title="About Me" key="about">About</a> <a href="https://theworkaround.com/posts" class="font-semibold text-gray-700 tracking-tight" title="View a list of all posts" key="posts">Posts</a> <a href="https://theworkaround.com/projects"" class="font-semibold text-gray-700 tracking-tight" title="Projects I'm working on" key="projects">Projects</a> <a href="https://theworkaround.com/feed.xml" class="flex content-center text-orange-600" title="RSS Feed" key="feed"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" className="h-5 w-5 self-center text-orange-600" class=" h-5 w-5 self-center text-orange-600" alt="RSS Feed Icon" aria-hidden="true" focusable="false" role="img"> <path d="M4 11a9 9 0 0 1 9 9"></path> <path d="M4 4a16 16 0 0 1 16 16"></path> <circle cx="5" cy="19" r="1"></circle> </svg> </a> </div> </nav> <main class="post px-10 sm:px-7 border-0"> <h1>Docker Swarm Persistent Storage</h1> <p>Unless you’ve been living under a rock, you should need no explanation what <a href="https://www.docker.com/">Docker</a> is. Using Docker over the last year has drastically improved my deployment ease and with coupled with <a href="https://about.gitlab.com/">GitLab’s</a> CI/CD has made deployment extremely ease. Mind you, not all our applications being deployed have the same requirements, some are extremely simple and others are extraordinarily complex. So when we start a new project we have a base docker build to begin from and based on the applications requirements we add/remove as needed.</p> <h3 id="a-little-about-docker-swarm">A little about Docker Swarm</h3> <p>For the large majority of most of our applications, having a volume associated with the deployed containers and storing information is the database fits the applications needs.</p> <p>In front of all our applications we used to use <a href="https://proxy.dockerflow.com/">Docker Flow Proxy</a> to quickly integrate our application into our deployed environment and assign it a subdomain based on it’s service. For a few months we experienced issues with the proxy hanging up, resources not being cleared, and lots of dropped connections. Since than I have rebuilt our docker infrastructure and now we use <a href="https://traefik.io/">Traefik</a> for our proxy routing and it has been absolutely amazing! It’s extremely fast, very robust and extensible, and easy to manipulate to fit your needs. Heck before even deploying it I was using <a href="https://docs.docker.com/compose/">docker-compose</a> to build a local network proxy to ensure it was what we needed. While Traefik was running in compose I was hitting domains such as <code class="highlighter-rouge">http://whoami.localhost/</code> and this was a great way to learn the basic configuration before pushing it into a staging/production swarm environment. <em>(That explaing how we got started with Traefik is a whole other post of it’s own.)</em></p> <p>Now back to our docker swarm, I know the big thing right now is <a href="https://kubernetes.io/">Kubernetes</a>. But every organization has their specific needs, for their different environments, application, types, and deployment mechanisms. In my opinion the current docker environment we’ve got running right now is pretty robust. We’ve got dozens of nodes, a number of deployment environments (cybersec, staging, and production), dozens of applications running at once, and some of then requiring a number of services in order to function properly.</p> <p>A few of the things that won me over on the docker swarm in the first place is it’s load balancing capabilities, it’s very fault-tolerant, and the self-healing mechanism that it uses in case a container crashes, a node locks up or drops, or a number of other issues. <em>(We’ve had a number of servers go down due to networking issues or a rack server crapping out and with the docker swarm running you could never even tel we were having issues as an end user to our applications.)</em></p> <p><em class="small">(Below is an image showing traffic hitting the swarm. If you have an application replicated upon deployment, traffic will be distributed amongst the nodes to prevent bottle necks.)</em></p> <p><img src="/images/posts/docker-swarm-persistent-storage/SwarmTraffic.svg" alt="Docker Swarm Traffic" class="img-fluid"/></p> <h3 id="why-would-you-need-persistent-storage">Why would you need persistent storage?</h3> <p>Since the majority of our applications are data orientated, (with most of them hitting several databases in a single request) we hadn’t really had to worry about persistent storage. This is because once we deployed the applications; their volumes held all of their required assets and any data they needed was fetched from the database.</p> <p>The easiest way to explain volumes, is when a container is deployed to a node (if specified) it will put aside a section of storage specifically for that container. For example say we have an application called DogTracker the was deployed on node A and B. This application can create and store files in their volumes on those nodes. But what happens when there’s an issue with the container on node A and the container cycles to node C? The data created by the container is left in the volume on node A an no longer available, until that applications container cycles back to node A.</p> <p>And from this arises the problem we began to face. We were starting to develop applications that were starting to require files to be shared amongst each other. We also have numerous applications that require files to be saved and distributed without them being dumped into the database as a blob. And these files were required to be available without cycling volumes and/or dumping them into the containers during build time. And because of this, we needed to be able to have some form of persistent and distributed file storage across our containers.</p> <p><em class="small">(Below is an image showing how a docker swarms volumes are oriented)</em></p> <p><img src="/images/posts/docker-swarm-persistent-storage/DockerSwarm.svg" alt="Docker Swarm Diagram" class="img-fluid"/></p> <h3 id="how-we-got-around-this">How we got around this!</h3> <p>Now in this day an age there’s got to be ways to get around this. There’s at least 101 ways to do just about anything and it doesn’t always have to be newest shiniest toy everyone’s using. I know saying this while using Docker is kind of a hypocritical statement, but shared file systems have been around for decades. You’ve been able to mount network drives, ftp drives, have organizational based shared folders, the list can go on for days.</p> <p>But the big question is, how do we get a container to mount a local shared folder or distribute volumes across all swarm nodes? Well, there’s a whole list of distributed filesystems and modern storage mechanisms in the <a href="https://docs.docker.com/engine/extend/legacy_plugins/">docker documentation</a>. Below is a list of the top recommended alternatives I found for <a href="https://en.wikipedia.org/wiki/Distributed_File_System_(Microsoft)">distributed file systems</a> or <a href="https://en.wikipedia.org/wiki/Network_File_System">NFS’s</a> for the docker stratosphere around container development.</p> <ul> <li><a href="https://ceph.com/">Ceph</a></li> <li><a href="https://github.com/rancher/convoy">Convoy</a></li> <li><a href="https://github.com/rexray/rexray">RexRay</a></li> <li><a href="https://portworx.com/use-case/docker-persistent-storage/">PortWorx</a></li> <li><a href="https://github.com/pvdbleek/storageos">StorageOS</a></li> <li><a href="http://www.xtreemfs.org/">xtreemfs</a></li> </ul> <p>I know you’re wondering why we didn’t use <a href="https://aws.amazon.com/s3/">S3</a>, <a href="https://www.digitalocean.com/products/spaces/">DigitalOcean Spaces</a>, <a href="https://cloud.google.com/storage/docs/">GCS</a>, or some other cloud storage. But internally we have a finite amount of resources and we can spin up VM’s and be rolling in a matter of moments. Especially considering we have build a number of <a href="https://www.ansible.com/">Ansible</a> playbooks to quickly provision our servers. Plus, why throw resources out on the cloud, when it’s not needed. Especially when we can metaphorically create our own network based file system and have our own cloud based storage system.</p> <p><em class="small">(Below is an image showing we want to distribute file system changes)</em></p> <p><img src="/images/posts/docker-swarm-persistent-storage/DockerSwarm_wStorage.svg" alt="" class="img-fluid"/></p> <p>After looking at several methods I settled on <a href="https://www.gluster.org/">GlusterFS</a> a scalable network filesystem. Don’t get me wrong, a number of the other alternatives are pretty ground breaking and some amazing work as been put into developing them. But I don’t have thousands of dollars to drop on setting up a network file system, that may or may not work for our needs. There were also several others that I did look pretty heavily into, such as <a href="https://github.com/pvdbleek/storageos">StorageOS</a> and <a href="https://ceph.com/">Ceph</a>. With StorageOS I really liked the idea of a container based file system that stores, synchronizing, and distributes files to all other storage nodes within the swarm. And it may just be me, but Ceph looked like the prime competitor to Gluster. They both have their <a href="https://technologyadvice.com/blog/information-technology/ceph-vs-gluster/">high points</a> and seem to work very reliable. But at the time; it wasn’t for me and after using Gluster for a few months, I believe that I made the right choice and it’s served it’s purpose well.</p> <p><a href="https://www.gluster.org/"><img src="/images/posts/docker-swarm-persistent-storage/gluster-ant.png" alt="Gluster Ant" class="img-fluid w-3/12"/></a></p> <h4 id="gluster-notes">Gluster Notes</h4> <p><em>(<strong>Note:</strong> The following steps are to be used on a Debian/Ubuntu based install.)</em></p> <p>Documentation for using Gluster can be found on their <a href="https://docs.gluster.org/en/latest/">docs</a>. Their installation instructions are very brief and explain how to install the gluster packages, but they don’t go into depth in how to setup a Gluster network. I also suggest thoroughly reading through to documentation to understand Gluster volumes, bricks, pools, etc.</p> <h3 id="installing-glusterfs">Installing GlusterFS</h3> <p>To begin you will need to list all of the Docker Swarm nodes you wish to connect in the <code class="highlighter-rouge">/etc/hosts</code> files of each server. On linux (Debian/Ubuntu), you can get the current nodes IP Address run the following command <code class="highlighter-rouge">hostname -I | awk '{print $1}'</code></p> <p><em class="fa fa-info-circle text-primary"> </em> <em class="small">(The majority of the commands listed below need to be ran on each and every node simultaneously unless specified. To do this I opened a number of terminal tabs and connected to each server in a different tab.)</em></p> <div class="language-config highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># /etc/hosts
</span><span class="m">10</span>.<span class="m">10</span>.<span class="m">10</span>.<span class="m">1</span> <span class="n">staging1</span>.<span class="n">example</span>.<span class="n">com</span> <span class="n">staging1</span>
<span class="m">10</span>.<span class="m">10</span>.<span class="m">10</span>.<span class="m">2</span> <span class="n">staging2</span>.<span class="n">example</span>.<span class="n">com</span> <span class="n">staging2</span>
<span class="m">10</span>.<span class="m">10</span>.<span class="m">10</span>.<span class="m">3</span> <span class="n">staging3</span>.<span class="n">example</span>.<span class="n">com</span> <span class="n">staging3</span>
<span class="m">10</span>.<span class="m">10</span>.<span class="m">10</span>.<span class="m">4</span> <span class="n">staging4</span>.<span class="n">example</span>.<span class="n">com</span> <span class="n">staging4</span>
<span class="m">10</span>.<span class="m">10</span>.<span class="m">10</span>.<span class="m">5</span> <span class="n">staging5</span>.<span class="n">example</span>.<span class="n">com</span> <span class="n">staging5</span>
</code></pre></div></div> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Update &amp; Upgrade all installed packages</span>
apt-get update <span class="o">&amp;&amp;</span> apt-get upgrade <span class="nt">-y</span>

<span class="c"># Install gluster dependencies</span>
<span class="nb">sudo </span>apt-get <span class="nb">install </span>python-software-properties <span class="nt">-y</span>
</code></pre></div></div> <p>Add the GlusterFS <a href="https://itsfoss.com/ppa-guide/">PPA</a> package the list of trusted packages to install from a community repository.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>add-apt-repository ppa:gluster/glusterfs-3.10<span class="p">;</span>
<span class="nb">sudo </span>apt-get update <span class="nt">-y</span> <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt-get update
</code></pre></div></div> <p>Now lets install gluster</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> glusterfs-server attr
</code></pre></div></div> <p>Now before starting the Gluster service but I had to copy some files into systemd <em>(you may or may not have to do this)</em>. But since Gluster was developed by <a href="https://www.redhat.com/en/technologies/storage/gluster">RedHat</a> primarily for <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">RedHat</a> and <a href="https://www.centos.org/">CentOS</a>, I had a few issues starting the system service.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo cp</span> /etc/init.d/glusterfs-server /etc/systemd/system/
</code></pre></div></div> <p>Let’s start and enable the glusterfs system service</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl <span class="nb">enable </span>glusterfs-server<span class="p">;</span> systemctl start glusterfs-server
</code></pre></div></div> <p>This step isn’t necessary, but I like to verify that</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Verify the gluster service is enabled</span>
systemctl is-enabled glusterfs-server
<span class="c"># Check the system service status of the gluster-server</span>
systemctl status glusterfs-server
</code></pre></div></div> <p>If for some reason you haven’t done this yet, each and every node should have it’s own ssh key generated.</p> <p><em class="small">(The only reason I can think of why they wouldn’t have a different key is if a VM was provisioned and than cloned for similar use across a swarm.)</em></p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># This is to generate a very basic SSH key, you may want to specify a key type such as ED25519 or bit length if required.</span>
ssh-keygen <span class="nt">-t</span> rsa
</code></pre></div></div> <p>Dependant on your Docker Swarm environment and which server you’re running as a manager; you’ll probably want one of the node managers to also be a gluster node manager as well. I’m going to say server <code class="highlighter-rouge">staging1</code> is one of our node managers, so on this server we’re going to probe all other gluster nodes to add them to the gluster pool. (Probing them essentially is saying this manager is telling all servers on this list to connect to each-other.)</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gluster peer probe staging1<span class="p">;</span> gluster peer probe staging2<span class="p">;</span> gluster peer probe staging3<span class="p">;</span> gluster peer probe staging4<span class="p">;</span> gluster peer probe staging5<span class="p">;</span>
</code></pre></div></div> <p>It’s not required, but probably good practice to ensure all of the nodes have connected to the pool before setting up the file system.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gluster pool list

<span class="c"># =&gt; You should get results similar to the following</span>
UUID					Hostname 	State
a8136a2b-a2e3-437d-a003-b7516df9520e	staging3 	Connected
2a2f93f6-782c-11e9-8f9e-2a86e4085a59	staging2 	Connected
79cb7ec0-f337-4798-bde9-dbf148f0da3b	staging4 	Connected
3cfc23e6-782c-11e9-8f9e-2a86e4085a59	staging5 	Connected
571bed3f-e4df-4386-bd46-3df6e3e8479f	localhost	Connected

<span class="c"># You can also run the following command to another set of results</span>
gluster peer status
</code></pre></div></div> <p>Now lets create the gluster data storage directories <em>(<strong>It’s very important you do this on every node.</strong> This is because this directory is where all gluster nodes will store the distributed files locally.)</em></p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo mkdir</span> <span class="nt">-p</span> /gluster/brick
</code></pre></div></div> <p>Now lets create a gluster volume across all nodes (again run this on the master node/node manager).</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>gluster volume create staging-gfs replica 5 staging1:/gluster/brick staging2:/gluster/brick staging3:/gluster/brick staging4:/gluster/brick staging5:/gluster/brick force
</code></pre></div></div> <p>The next step is to initialize the glusterFS to begin synchronizing across all nodes.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gluster volume start staging-gfs
</code></pre></div></div> <p>This step is also not required, but I prefer to verify the gluster volume replicated across all of the designated nodes.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gluster volume info
</code></pre></div></div> <p>No let’s ensure we have gluster mount the <code class="highlighter-rouge">/mtn</code> directory for it’s shared directory especially on a reboot. <strong><em>(It’s important to run these commands on all gluster nodes.)</em></strong></p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>umount /mnt
<span class="nb">sudo echo</span> <span class="s1">'localhost:/staging-gfs /mnt glusterfs defaults,_netdev,backupvolfile-server=localhost 0 0'</span> <span class="o">&gt;&gt;</span> /etc/fstab
<span class="nb">sudo </span>mount.glusterfs localhost:/staging-gfs /mnt
<span class="nb">sudo chown</span> <span class="nt">-R</span> root:docker /mnt
</code></pre></div></div> <p><em class="small">(You may have noticed the setting of file permissions using <code class="highlighter-rouge">chown -R root:docker</code> this is to ensure docker will have read/write access to the files in the specified directory.)</em></p> <p>If for some reason you’ve already deployed your staging gluster-fs and need to remount the staging-gfs volume you can run the following command. Otherwise you should be able to skip this step.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>umount /mnt<span class="p">;</span> <span class="nb">sudo </span>mount.glusterfs localhost:/staging-gfs /mnt<span class="p">;</span> <span class="nb">sudo chown</span> <span class="nt">-R</span> root:docker /mnt
</code></pre></div></div> <p>Let’s list all of our mounted partitions and ensure that the <code class="highlighter-rouge">staging-gfs</code> is listed.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">df</span> <span class="nt">-h</span>

<span class="c"># =&gt; staging-gfs should be listed in the partitions/disks listed</span>
localhost:/staging-gfs              63G   13G   48G  21% /mnt
</code></pre></div></div> <p>Now that all of the work is pretty much done, now comes the fun part lets test to make sure it all works. Lets <code class="highlighter-rouge">cd</code> into the <code class="highlighter-rouge">/mnt</code> directory and create a few files to make sure they will sync across all nodes. <em>(I know this is one of the first things I wanted to try out.)</em> You can do one of the following commands to generate a random file in the <code class="highlighter-rouge">/mnt</code> directory. Now depending on your servers and network connections this should sync up across all nodes almost instantly. The way I tested this I was in the <code class="highlighter-rouge">/mtn</code> directory on several nodes in several terminals. And as soon as I issued the command I was running the <code class="highlighter-rouge">ls</code> command in the other tabs. And depending on the file size, it may not sync across all nodes instantly, but is at least accessible.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># This creates a 24MB file full of zeros</span>
<span class="nb">dd </span><span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>output.dat <span class="nv">bs</span><span class="o">=</span>24M  <span class="nv">count</span><span class="o">=</span>1

<span class="c"># Creates a 2MB file of random characters</span>
<span class="nb">dd </span><span class="k">if</span><span class="o">=</span>/dev/urandom <span class="nv">of</span><span class="o">=</span>output.log <span class="nv">bs</span><span class="o">=</span>1M <span class="nv">count</span><span class="o">=</span>2
</code></pre></div></div> <h3 id="using-glusterfs-with-docker">Using GlusterFS with Docker</h3> <p>Now that all the fun stuff is done if you have looked at docker <a href="https://docs.docker.com/storage/volumes/">volumes</a> or <a href="https://docs.docker.com/storage/bind-mounts/">bind</a> mounts this would probably be a good time. Usually docker will store a volumes contents in a folder structure similar to the following: <code class="highlighter-rouge">/var/lib/docker/volumes/DogTracker/_data</code>.</p> <p>But in your <code class="highlighter-rouge">docker-compose.yml</code> or <code class="highlighter-rouge">docker-stack.yml</code> you can specify specific mount points for the docker volumes. If you look at the following <a href="https://en.wikipedia.org/wiki/YAML">YAML</a> snippet you will notice I’m saying to store the containers <code class="highlighter-rouge">/opt/couchdb/data</code> directory on the local mount point <code class="highlighter-rouge">/mnt/staging_couch_db</code>.</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3.7'</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">couchdb</span><span class="pi">:</span>
  <span class="na">image</span><span class="pi">:</span> <span class="s">couchdb:2.3.0</span>
  <span class="na">volumes</span><span class="pi">:</span>
   <span class="pi">-</span> <span class="na">type</span><span class="pi">:</span> <span class="s">bind</span>
     <span class="na">source</span><span class="pi">:</span> <span class="s">/mnt/staging_couch_db</span>
     <span class="na">target</span><span class="pi">:</span> <span class="s">/opt/couchdb/data</span>
  <span class="na">networks</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">internal</span>
  <span class="na">deploy</span><span class="pi">:</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">limits</span><span class="pi">:</span>
        <span class="na">cpus</span><span class="pi">:</span> <span class="s1">'</span><span class="s">0.30'</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s">512M</span>
      <span class="na">reservations</span><span class="pi">:</span>
        <span class="na">cpus</span><span class="pi">:</span> <span class="s1">'</span><span class="s">0.15'</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s">256M</span>
</code></pre></div></div> <p>Now as we had previously demonstrated any file(s) saved, created, and/or deleted in the <code class="highlighter-rouge">/mtn</code> directory will be synchronized across all of the GlusterFS nodes.</p> <p>I’d just like to mention this may not work for everyone, but this is the method that worked best for use. We’ve been running a number of different Gluster networks for several months now with no issues <em>thus far</em>.</p> </main> <footer class="space-y-3 text-sm text-center"> <p> Contact me at <a href="mailto:tarellel@gmail.com">tarellel@gmail.com</a> <p/> <p> &copy; 2004 - 2023 Brandon Hicks - All rights reserved.<br> Content on this site is licensed under the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons 4.0 License</a>.<br> Code samples and snippets are licensed under the <a href="https://github.com/tarellel/theworkaround.com/blob/main/LICENSE">MIT License</a> </p> </footer> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-41384218-1"></script> <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-41384218-1');
    </script> </body> </html>